{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maching learning flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='https://scikit-learn.org/stable/_static/ml_map.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification-- Decisition tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 决策树和信息熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话:\n",
    "    \n",
    "    女儿：多大年纪了？\n",
    "\n",
    "    母亲：26。\n",
    "\n",
    "    女儿：长的帅不帅？\n",
    "\n",
    "    母亲：挺帅的。\n",
    "\n",
    "    女儿：收入高不？\n",
    "\n",
    "    母亲：不算很高，中等情况。\n",
    "\n",
    "    女儿：是公务员不？\n",
    "\n",
    "    母亲：是，在税务局上班呢。\n",
    "\n",
    "    女儿：那好，我去见见。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images2015.cnblogs.com/blog/456673/201509/456673-20150929222630386-179635683.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1：\n",
    "    首先，我们该选择什么标准(属性、特征)作为我们的首要条件(根节点)对样本(男人)进行划分，决定见或不见呢？\n",
    "\n",
    "    \n",
    "答1：\n",
    "    我们需要获得尽可能多的信息，减少不确定性。母亲得到信息越多，女儿的态度越明确，与男方见与不见的不确定性越低。\n",
    "    \n",
    "    \n",
    "问题2：\n",
    "    那么我们如何去衡量信息量呢？\n",
    "    \n",
    "答2：\n",
    "    信息量的大小是和这件事发生的概率相关的，当事件发生的概率越低，传递的信息量越大，必然发生的信息量为0。我们引入信息熵的概念来衡量信息不确定的度量，信息熵——信息熵的计算就是信息量的数学期望，特点如下：\n",
    "    \n",
    "        （1）信息熵与事件的可能性数量有关，在概率均等的情况下，存在的可能越多，信息熵越大，信息也越不确定；\n",
    "\n",
    "        （2）信息熵与事件的概率分布情况有关，概率分布越平均，信息熵越大，当所有概率均等的情况下，信息熵达到最大；\n",
    "\n",
    "\n",
    "数学定义：\n",
    "    如果一件事有k种可能的结果，每种结果的概率为  $P_i,i=1,...,k$,则我们对此事件的结果进行观察后的信息量为\n",
    "    \n",
    "$$\n",
    "I=-(P_1 log_2 P_1+ P_2 log_2 P_2 + ···+P_k log_2 P_k)=-\\sum_{i=1}^{k} P_i log_2 P_i\n",
    "$$\n",
    "\n",
    "信息熵的公式是怎么来的：https://www.zhihu.com/question/22178202 .\n",
    "  \n",
    "</br>  \n",
    "条件熵：条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。例如，知道男生年龄的前提条件下，根据女儿见与不见的不确定性。\n",
    "\n",
    "信息增益：信息增益表示得知特征X(年龄)的信息使得类Y(见与不见)的信息的不确定性减少程度。信息增益大表明信息增多,信息增多，则不确定性就越小，母亲应该选择使得信息增益增大的条件询问女儿。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img-blog.csdn.net/20161011163342038?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ID3算法\n",
    "\n",
    "   ID3由Ross Quinlan在1986年提出。ID3决策树可以有多个分支，但是不能处理特征值为连续的情况。\n",
    "   \n",
    "   </br>\n",
    "   决策树是一种贪心算法，每次选取的分割数据的特征都是当前的最佳选择，并不关心是否达到最优。在ID3中，每次根据“最大信息熵增益”选取当前最佳的特征来分割数据，并按照该特征的所有取值来切分，也就是说如果一个特征有4种取值，数据将被切分4份，一旦按某特征切分后，该特征在之后的算法执行中，将不再起作用，所以有观点认为这种切分方式过于迅速。\n",
    "   \n",
    "   </br>\n",
    "   ID3算法十分简单，核心是根据“最大信息熵增益”原则选择划分当前数据集的最好特征，根据特征属性划分数据，使得原本“混乱”的数据的熵(混乱度)减少，按照不同特征划分数据熵减少的程度会不一样。在ID3中选择熵减少程度最大的特征来划分数据（贪心），也就是“最大信息熵增益”原则。\n",
    "   \n",
    "</br>\n",
    "\n",
    "1）计算数据集D的信息熵：\n",
    "$$\n",
    "H(D)=-\\sum_{k=1}^{K}\\frac{|D_k|}{|D|}log_2 \\frac{|D_k|}{|D|}\n",
    "$$\n",
    "公式中$\\frac{|D_k|}{|D|}$表示第K类样本占数据集D样本总数的比例\n",
    "\n",
    "2）计算特征A对数据集D的条件熵 H(D|A)：\n",
    "$$\n",
    "H(D|A)=\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}\\sum_{k=1}^{K}\\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}\n",
    "$$\n",
    "公式（2）表示的是以特征A作为分割的属性，得到的信息熵。因此该公式求的是以属性A为划分，n个分支的信息熵总和。\n",
    "\n",
    "3)计算信息增益\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ID3算法的不足"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。　　\n",
    "\n",
    "　　　　a)ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。\n",
    "\n",
    "　　　　b)ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如果校正这个问题呢？\n",
    "\n",
    "　　　　c) ID3算法对于缺失值的情况没有做考虑\n",
    "\n",
    "　　　　d) 没有考虑过拟合的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 C4.5算法\n",
    "C4.5是Ross Quinlan在1993年在ID3的基础上改进而提出的。ID3采用的信息增益度量存在一个缺点，**它一般会优先选择有较多属性值的Feature**,因为属性值多的Feature会有相对较大的信息增益。(信息增益反映的给定一个条件以后不确定性减少的程度,必然是分得越细的数据集确定性更高,也就是条件熵越小,信息增益越大).为了避免这个不足C4.5中是用信息增益比率(gain ratio)来作为选择分支的准则。信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的Feature,也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它。\n",
    "\n",
    "除此之外，C4.5还弥补了ID3中不能处理特征属性值连续的问题。但是，对连续属性值需要扫描排序，会使C4.5性能下降\n",
    "\n",
    "**信息增益率公式：**\n",
    "$$\n",
    "SplitInformation(D,A)=-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}log_2 \\frac{|D_i|}{|D|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "GainRatio(D,A)=\\frac{g(D,A)}{SplitInformation(D,A)}\n",
    "$$\n",
    "\n",
    "上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性。\n",
    "\n",
    "注：但增益率也可能产生一个问题就是，对可取数值数目较少的属性有所偏好。因此，C4.5算法并不是直接选择使用增益率最大的候选划分属性，而是使用了一个启发式算法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益率最高的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5算法的改进\n",
    "对于缺失值处理的问题，主要需要解决的是两个问题，\n",
    "\n",
    "    一是在样本某些特征缺失的情况下选择划分的属性，\n",
    "    二是选定了划分属性，对于在该属性上缺失特征的样本的处理。\n",
    "\n",
    "对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。\n",
    " \n",
    "对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 CART算法\n",
    "CART（Classification and Regression tree）分类回归树由L.Breiman,J.Friedman,R.Olshen和C.Stone于1984年提出。\n",
    "\n",
    "    ID3中根据属性值分割数据，之后该特征不会再起作用，这种快速切割的方式会影响算法的准确率。\n",
    "    \n",
    "    CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。\n",
    " \n",
    "    CART中每一次迭代都会降低GINI系数。回归时使用均方差作为loss function。基尼系数的计算与信息熵增益的方式非常类似\n",
    "\n",
    "$$\n",
    "Gini(D)=1-\\sum_{k=1}^{K}(\\frac{|D_k|}{|D|})^2=\\sum_{k=1}^{K}p_k(1-p_k)\n",
    "$$\n",
    "\n",
    "从公式可以看到，基尼指数的意义是：从数据集D中随机抽取两个样本，其类别标记不一致的概率。直觉地，基尼指数越小，则数据集D的纯度越高。\n",
    "\n",
    "$$\n",
    "Gain\\_Gini(D,A)=\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}Gini(D_i)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART算法的改进\n",
    "CART回归树和CART分类树的建立和预测的区别主要有下面两点：\n",
    "\n",
    "    　\n",
    "     1)连续值的处理方法不同 ——对于连续值的处理，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。\n",
    "     \n",
    "     2)决策树建立后做预测的方式不同——对于决策树建立后做预测的方式，上面讲到了CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table class=\"table table-bordered table-striped table-condensed\">\n",
    "<tr> \n",
    "    <th style=\"width: 100px;text-align:center;\">算法</th> \n",
    "    <th style=\"width: 100px;text-align:center;\">支持模型</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">树结构</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">特征选择</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">连续值处理</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">缺失值处理</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">剪枝</th>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "    <th style=\"width: 100px;text-align:center;\">ID3</th> \n",
    "    <th style=\"width: 100px;text-align:center;\">分类</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">多叉树</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">信息增益</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">不支持</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">不支持</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">不支持</th>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "    <th style=\"width: 100px;text-align:center;\">C4.5</th> \n",
    "    <th style=\"width: 100px;text-align:center;\">分类</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">多叉树</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">信息增益比</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">支持</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">支持</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">支持</th>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr> \n",
    "    <th style=\"width: 100px;text-align:center;\">CART</th> \n",
    "    <th style=\"width: 100px;text-align:center;\">分类,回归</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">二叉树</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">基尼系数，均方差</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">支持</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">支持</th>\n",
    "    <th style=\"width: 100px;text-align:center;\">支持</th>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例\n",
    "编号 | 年龄|  长相| 工资 | 编程 | 类别\n",
    "-|-|-|-|-|-\n",
    "  1  |  老 |  帅  | 高   | 不会 | 不见\n",
    "  2  | 年轻| 一般 | 中等 |  会  | 见\n",
    "  3  | 年轻|  丑  | 高   | 不会 | 不见\n",
    "  4  | 年轻| 一般 | 高   |  会  | 见\n",
    "  5  | 年轻| 一般 | 低   | 不会 | 不见\n",
    "\n",
    "### ID3\n",
    "以表中相亲数据为例，该数据包含5个样本集，正样本占比$\\frac{2}{5}$，负样本占比$\\frac{3}{5}$。于是，根据公式计算出根节点的信息熵为：\n",
    "\n",
    "$$\n",
    "H(D)=-\\frac{3}{5} log_2 \\frac{3}{5} -\\frac{2}{5} log_2 \\frac{2}{5}=0.971\n",
    "$$\n",
    "\n",
    "然后，计算当前属性集合{年龄，长相，工资，编程}中每个属性的条件熵。\n",
    "\n",
    "以属性“年龄”为例，它有2个可能的取值：{老，年轻}。若使用该属性对D进行划分，则可得到2个子集，分别为：$D_1$(年龄=老)，$D_2$ (年龄=年轻)。\n",
    "\n",
    "子集$D_1$包含编号{1}的1个样例，其中正样本占比$0$，负样本占比$1$；\n",
    "子集$D_2$包含编号{2，3，4，5}的4个样例，其中正样本占比$2/4$，负样本占比$2/4$；\n",
    "根据公式计算出属性=年龄的条件熵为：\n",
    "\n",
    "$$\n",
    "H(D|年龄)=\\frac{1}{5}H(D_1|老)+\\frac{4}{5}H(D_2|年轻)=\\frac{1}{5}(-0)+\\frac{4}{5}(-\\frac{2}{4}log_2 \\frac{2}{4} -\\frac{2}{4} log_2\\frac{2}{4})=0.8\n",
    "$$\n",
    "依此类推：\n",
    "\n",
    "$H(D|长相)=\\frac{1}{5}H(帅)+\\frac{3}{5}H(一般)+\\frac{1}{5}H(丑)=0.551$\n",
    "\n",
    "$H(D|工资)=\\frac{3}{5}H(高)+\\frac{1}{5}H(中等)+\\frac{1}{5}H(低)=0.551$\n",
    "\n",
    "$H(D|编程)=\\frac{3}{5}H(不会)+\\frac{2}{5}H(会)=0$\n",
    "\n",
    "每个属性对应的信息增益为：\n",
    "g(D,年龄)=0.171，g(D,长相)=0.42，g(D,工资)=0.42，g(D,编程)=0.971，由此可得，特征“编程”的信息增益最大，使用特征“编程”来进行划分所得的纯度提升越大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5\n",
    "在ID3的基础上，计算每个数据集划分的分裂信息项，由 $SI=-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}log_2 \\frac{|D_i|}{|D|}$\n",
    "\n",
    "$SI_{年龄}=-\\frac{1}{5}log_2\\frac{1}{5}-\\frac{4}{5}log_2\\frac{4}{5}=0.722$, \n",
    "$SI_{长相}=-\\frac{1}{5}log_2\\frac{1}{5}-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{1}{5}log_2\\frac{1}{5}=1.371$, \n",
    "$SI_{工资}=-\\frac{1}{5}log_2\\frac{1}{5}-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{1}{5}log_2\\frac{1}{5}=1.371$, \n",
    "$SI_{编程}=-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}=0.971$\n",
    "\n",
    "最终可计算出各个特征的信息增益比：\n",
    "\n",
    "$GR(D,年龄)=\\frac{g(D,年龄)}{SI_{年龄}}=0.2386$,\n",
    "$GR(D,长相)=\\frac{g(D,长相)}{SI_{长相}}=0.3063$,\n",
    "$GR(D,工资)=\\frac{g(D,工资)}{SI_{工资}}=0.3063$,\n",
    "$GR(D,编程)=\\frac{g(D,编程)}{SI_{编程}}=1$\n",
    "\n",
    "通过信息增益比，特征年龄对应的指标上升了，而特征长相和工资有所下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART \n",
    "\n",
    "使用CART分类准则，选取年龄维度，把老作为特征标签，那么年轻就被划分到另外一类\n",
    "\n",
    "$$\n",
    "Gain\\_Gini(D|年龄=老)=\\frac{1}{5}(1-(\\frac{1}{1})^2-(\\frac{0}{1})^2)+\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)=0.4\n",
    "$$\n",
    "\n",
    "$$\n",
    "Gain\\_Gini(D|年龄=年轻)=\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)+\\frac{1}{5}(1-(\\frac{1}{1})^2-(\\frac{0}{1})^2)=0.4\n",
    "$$\n",
    "\n",
    "选取长相帅作为特征标签,那么一般和丑被划分到另一类\n",
    "\n",
    "$$\n",
    "Gain\\_Gini(D|长相=帅)=\\frac{1}{5}(1-(\\frac{1}{1})^2-(\\frac{0}{1})^2)+\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)=0.4\n",
    "$$\n",
    "\n",
    "选取长相一般作为特征标签,那么丑和帅被划分到另一类\n",
    "$$\n",
    "Gain\\_Gini(D|长相=一般)=\\frac{3}{5}(1-(\\frac{1}{3})^2-(\\frac{2}{3})^2)+\\frac{2}{5}(1-(\\frac{0}{2})^2-(\\frac{2}{2})^2)=0.27\n",
    "$$\n",
    "\n",
    "选取工资高作为特征标签,那么低和中等被划分到另一类\n",
    "\n",
    "$$\n",
    "Gain\\_Gini(D|工资=高)=\\frac{3}{5}(1-(\\frac{1}{3})^2-(\\frac{2}{3})^2)+\\frac{2}{5}(1-(\\frac{1}{2})^2-(\\frac{1}{2})^2)=0.47\n",
    "$$\n",
    "\n",
    "选取工资中等作为特征标签,那么低和高等被划分到另一类\n",
    "$$\n",
    "Gain\\_Gini(D|工资=中等)=\\frac{1}{5}(1-(\\frac{1}{1})^2-(\\frac{0}{1})^2)+\\frac{4}{5}(1-(\\frac{1}{4})^2-(\\frac{3}{4})^2)=0.3\n",
    "$$\n",
    "\n",
    "选取工资低作为特征标签,那么高和中等被划分到另一类\n",
    "$$\n",
    "Gain\\_Gini(D|工资=低)=\\frac{1}{5}(1-(\\frac{1}{1})^2-(\\frac{0}{1})^2)+\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)=0.4\n",
    "$$\n",
    "\n",
    "选取会编程作为特征标签,那么不会编程被划分到另一类\n",
    "\n",
    "$$\n",
    "Gain\\_Gini(D|编程=会)=\\frac{2}{5}(1-(\\frac{2}{2})^2-(\\frac{0}{2})^2)+\\frac{3}{5}(1-(\\frac{3}{3})^2-(\\frac{0}{3})^2)=0\n",
    "$$\n",
    "\n",
    "因此，特征编程的Gini指数最小，选择该特征作为最优的切分点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jianshu.com/p/2c893840687a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 决策树的优点和缺点\n",
    "    首先我们看看决策树算法的优点：\n",
    "\n",
    "　　　　1）简单直观，生成的决策树很直观。\n",
    "\n",
    "　　　　2）基本不需要预处理，不需要提前归一化，处理缺失值。\n",
    "\n",
    "　　　　3）使用决策树预测的代价是O(log2m)。 m为样本数。\n",
    "\n",
    "　　　　4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。\n",
    "\n",
    "　　　　5）可以处理多维度输出的分类问题。\n",
    "\n",
    "　　　　6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释\n",
    "\n",
    "　　　　7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。\n",
    "\n",
    "　　　　8） 对于异常点的容错能力好，健壮性高。\n",
    "\n",
    "　   我们再看看决策树算法的缺点:\n",
    "\n",
    "　　　　1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。\n",
    "\n",
    "　　　　2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。\n",
    "\n",
    "　　　　3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。\n",
    "\n",
    "　　　　4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。\n",
    "\n",
    "　　　　5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Improving classification with the AdaBoost meta-algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Building classifiers from randomly resampled data: bagging\n",
    "Bootstrap aggregating: \n",
    "\n",
    "data is taken from the original dataset S times to make S new datasets\n",
    "\n",
    "After the S datasets are built, a learning algorithm is applied to each one individually\n",
    "\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/4251424-07fe7d6f536c35b4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)\n",
    "\n",
    "## 2.1.2 Boosting\n",
    "In boosting and bagging, you always use\n",
    "the same type of classifier. But in boosting, the different classifiers are trained sequentially.\n",
    "\n",
    "**Boosting is different from bagging because the output is calculated from a\n",
    "weighted sum of all classifiers. The weights aren’t equal as in bagging but are based on\n",
    "how successful the classifier was in the previous iteration.**\n",
    "\n",
    "\n",
    "<img src='https://upload-images.jianshu.io/upload_images/4251424-14971af460b9e68b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Bagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。\n",
    "\n",
    "Boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。\n",
    "\n",
    "**Bagging和Boosting的区别：**\n",
    "\n",
    "1）样本选择上：\n",
    "\n",
    "Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\n",
    "\n",
    "Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n",
    "\n",
    "2）样例权重：\n",
    "\n",
    "Bagging：使用均匀取样，每个样例的权重相等\n",
    "\n",
    "Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n",
    "\n",
    "3）预测函数：\n",
    "\n",
    "Bagging：所有预测函数的权重相等。\n",
    "\n",
    "Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n",
    "\n",
    "4）并行计算：\n",
    "\n",
    "Bagging：各个预测函数可以并行生成\n",
    "\n",
    "Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Adaboost 原理理解\n",
    "AdaBoost算法：\n",
    "    \n",
    "输入：训练数据集 $T={(x_1,y_1),(x_2,y_2),···,(x_N,y_N)}$ ；弱学习算法\n",
    "\n",
    "输出：分类器 G(x)\n",
    "    \n",
    "1.初始化训练数据的权值分布\n",
    "$$\n",
    "D_1=(w_{11},w_{12},···,w_{1N}), w_{1i}=\\frac{1}{N}, i=1,2,···，N\n",
    "$$  \n",
    "    \n",
    "    我们有一个数据集，样本大小为N，每一个样本对应一个原始标签起初，我们初始化样本的权重为1/N\n",
    "\n",
    "2.对 $m=1,2,···,M$\n",
    "\n",
    "    2.1 使用具有权值分布D_m的训练数据集学习，得到基本分类器\n",
    "\n",
    "$$\n",
    "G_m(x): x →\\{-1,1\\}\n",
    "$$\n",
    "\n",
    "    2.2 计算G_m(x)在训练数据集上的分类误差率 \n",
    "$$\n",
    "e_m=\\sum_{i=1}^{N}P(G_m(x_i)≠ y_i)\n",
    "$$\n",
    "\n",
    "    2.3 计算G_m(x)的系数 \n",
    "$$\n",
    "\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}\n",
    "$$\n",
    "\n",
    "    2.4 更新训练数据集的权值分布\n",
    "$$\n",
    "D_m=(w_{m+1,1},···,w_{m+1,i},···,w_{m+1,N})\n",
    "$$\n",
    "\n",
    "\n",
    "3.构建基本分类器的线性组合\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^{M}\\alpha_mG_m(x)\n",
    "$$\n",
    "\n",
    "得到最终分类器\n",
    "\n",
    "$$\n",
    "G(x)=sign(f(x))=sign(\\sum_{m=1}^{M}\\alpha_mG_m(x))\n",
    "$$\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于Boosting的理解，对于AdaBoost，我们要搞清楚两点：\n",
    "\n",
    "    1）每一次迭代的弱学习h(x;am) 有何不一样，如何学习？\n",
    "\n",
    "    2）弱分类器权值αm 如何确定？\n",
    "\n",
    "对于第一个问题，AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。然后，再根据所采用的一些基本机器学习算法进行学习，比如逻辑回归。\n",
    "\n",
    "对于第二个问题，AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。\n",
    "\n",
    "Adaboost算法流程是什么\n",
    "\n",
    "    1）给数据中的每一个样本一个权重\n",
    "    2）训练数据中的每一个样本，得到第一个分类器\n",
    "    3）计算该分类器的错误率，根据错误率计算要给分类器分配的权重（注意这里是分类器的权重）\n",
    "    4）将第一个分类器分错误的样本权重增加，分对的样本权重减小（注意这里是样本的权重）\n",
    "    5）然后再用新的样本权重训练数据，得到新的分类器，到步骤3\n",
    "    6）直到步骤3中分类器错误率为0，或者到达迭代次数\n",
    "    7）将所有弱分类器加权求和，得到分类结果（注意是分类器权重）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left; padding-right: 100px;\"><img src=\"https://pic3.zhimg.com/80/v2-be394857423bcc569e2ef7cb2faca8de_hd.jpg\"  width=\"200\" height=\"200\" ></div>\n",
    "<div style=\"float:left; padding-right: 100px;\"><img src=\"https://pic4.zhimg.com/80/v2-03ba2bd553fd52373861a2bfde7e2763_hd.jpg\" width=\"200\" height=\"200\" ></div>\n",
    "<div style=\"float:left; padding-right: 10px;\"><img src=\"https://pic1.zhimg.com/80/v2-4d6155833121aabd3b67a44986f64a10_hd.jpg\" width=\"240\" height=\"200\" ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left; padding-right: 20px; padding-top: 40px;\"><img src=\"https://pic1.zhimg.com/v2-2055341db24c035b7287641824ca6d34_r.jpg\"  width=\"600\" height=\"300\" ></div>\n",
    "<div style=\"float:left; padding-right: 10px;\"><img src=\"https://pic2.zhimg.com/80/v2-759ae2e46f5940cdb425dec0f86f44b5_hd.jpg\" width=\"240\" height=\"200\" ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adaboost和RandomForest对比\n",
    "Adaboost算法常用的弱学习器是决策树和神经网络（理论上可以用任何学习器作为基学习器）。对于决策树，Adaboost分类用了CART分类树，Adaboost回归用了CART回归树。Adaboost和Boosting算法一样，重点关注模型的偏差，迭代的过程中是以降低偏差为目的，一般偏差会较小，但这并不意味这Adaboost的方差很大，很容易过拟合，实际上是可以通过调整模型的复杂度来避免过拟合的，例如决策树为基学习器时，可以调节树深，或者是叶节点中样本的个数等来实现。Adaboost只能做二分类问题，最终的输出是用sign函数决定的。当然对算法上做一些修改也是可以用于回归问题的，但是对于多分类问题就比较复杂。\n",
    "\n",
    "　　Adaboost的主要优点：\n",
    "\n",
    "　　1）Adaboost作为分类器时，分类精度很高\n",
    "\n",
    "　　2）在Adaboost的框架下，可以使用各种分类回归模型来构建基学习器\n",
    "\n",
    "　　3）作为简单的二分类问题，构建简单，结果可理解\n",
    "\n",
    "　　4）不容易发生过拟合\n",
    "\n",
    "　　Adaboost的主要缺点：\n",
    "\n",
    "　　1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习其的预测准确性\n",
    "\n",
    "　　2）Adaboost只能做二分类问题，要做多分类问需要做其他的变通\n",
    "\n",
    "　　RandomForest算法常用的弱学习器也是决策树和神经网络，对于决策树，在随机森林中也是使用CART分类回归树来处理分类回归问题，通常采用投票法和平均法来决定最后的输出，对于分类问题采用投票法也决定了随机森林能无修改的应用于多分类问题，随机森林和Bagging算法一样，重点关注降低模型的方差，泛化能力强，但是有时候也会出现较大的训练误差，这可以通过加大模型的复杂度来解决，另外在随机森林中是采用随机选择子特征集的，子特征集的个数也会影响要模型的方差和偏差，一般认为子特征集越大，模型的偏差会越小。因为随机森林的模型简单，效果好，因此也产生了很多变种算法，这些变种算法可以用来处理分类回归问题，也可以处理特征转换，异常点检测等。\n",
    "\n",
    "　　例如extra trees就是随机森林的一种推广形式，它改变了两个地方，一是不再采取自助采样法去随机抽取样本，而是采用原始集作为训练样本；二是直接随机选择特征（相当于子特征集只有一个元素）。extra trees的方差比随机森林更小，因此泛化能力更强，但是偏差也更大。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　随机森林的主要优点：\n",
    "\n",
    "　　1）训练可以高度并行化，因此算法的速度要选快于Adaboost。\n",
    "\n",
    "　　2）由于随机子特征集，因此在高维特征下，算法仍具有较好的效率\n",
    "\n",
    "　　3）在训练后可以给出各个特征对输出的重要性\n",
    "\n",
    "　　4）由于随机采样，训练出的模型的方差小，泛化能力强\n",
    "\n",
    "　　5）算法实现起来比Boosting更简单\n",
    "\n",
    "　　6）对部分特征缺失不敏感\n",
    "\n",
    "　　随机森林的主要缺点：\n",
    "\n",
    "　　1）在某些噪声比较大的样本集上，RF模型容易陷入过拟合\n",
    "\n",
    "　　2）取值比较多的特征容易影响随机森林的决策，影响模型的拟合效果\n",
    "\n",
    "　　最后关于Bagging重点降低方差，Boosting重点降低偏差来说一说。首先对于Bagging来说，利用对样本进行重采样，然后用类似的模型来训练基学习器，由于子样本集的相似性和使用的模型相同，因此最终的基学习器有近似相等的方差和偏差，对于类似的偏差是无法通过模型平均来降低的，而对于方差在模型平均时会弱化每个基学习器的异常值，强化每个基学习器的共通值，因此会降低模型的方差。对于Boosting，其算法的核心就是关注偏差，在算法优化过程中每次都最小化弱学习器，这本身就是一种以降低偏差为目的的优化方式，因此会降低模型的偏差，当然Boosting也会降低模型的方差，只是没有偏差明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 GBDT\n",
    "GBDT是以决策树（CART）为基学习器的GB算法，是迭代树，而不是分类树。Boost是\"提升\"的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。GBDT的核心就在于：\n",
    "\n",
    "    每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。\n",
    "    \n",
    "GBDT 算法可以看成是由 K 棵树组成的加法模型,模型可描述为：\n",
    "$$\n",
    "\\hat{y_i}=\\sum_{k=1}^{K}f_k(x_i),  \\qquad f_k ∈ F\n",
    "$$\n",
    "\n",
    "迭代过程如下：\n",
    "$$\n",
    "\\hat{y}_i^t=\\sum_{k=1}^{t}f_k(x_i)=\\hat{y}_i^{t-1}+f_t(x_i)\n",
    "$$\n",
    "\n",
    "在第t步，这个时候的目标函数可以表示为：\n",
    "\n",
    "$$\n",
    "Obj^{(t)}=损失函数=\\sum_{i=1}^{n}L(y_i,\\hat{y}_i^t)=\\sum_{i=1}^{n}L(y_i,\\hat{y}_i^{t-1}+f_t(x_i))\n",
    "$$\n",
    "\n",
    "特别地，若损失函数为平方损失，则目标函数为\n",
    "\n",
    "$$\n",
    "Obj^{(t)}=\\sum_{i=1}^{n}(y_i-(\\hat{y}_i^{t-1}+f_t(x_i)))^2=\\sum_{i=1}^{n}(2(\\hat{y}_i^{t-1}-y_i)f_t(x_i)+f_t(x_i)^2)\n",
    "$$\n",
    "\n",
    "### GBDT 算法流程：\n",
    "GBDT的求解过程就是梯度下降在函数空间中的优化过程\n",
    "\n",
    "1.由函数的一阶泰勒展开\n",
    "$$\n",
    "f(\\theta_{k+1})\\approx f(\\theta_{k})+\\frac{\\partial f(\\theta_{k})}{\\partial\\theta_{k}}(\\theta_{k+1}-\\theta_{k})\n",
    "$$\n",
    "\n",
    "则 优化函数 f时, $\\theta_{k+1}=\\theta_{k}-\\eta \\frac{\\partial f(\\theta_k)}{\\partial \\theta_k}$\n",
    "\n",
    "2.对GBDT 的损失函数做泰勒展开：\n",
    "\n",
    "$$\n",
    "L(y,F_m(x))\\approx L(y,F_{m-1}(x))+\\frac{\\partial L(y,F_{m-1}(x))}{\\partial F_{m-1}(x)}(F_m(x)-F_{m-1}(x))\n",
    "$$\n",
    "\n",
    "则 优化函数 $L(y,F(x))$时, $F_{m}(x)=F_{m-1}(x)-\\eta\\frac{\\partial L(y,F_{m-1}(x))}{\\partial F_{m-1}(x)}$\n",
    "\n",
    "在1和2中都是随机梯度下降，但是不同的是：1在参数空间中优化，每次迭代得到参数的增量，这个增量就是负梯度乘上学习率；2在函数空间中优化，每次得到增量函数，这个函数会去拟合负梯度，在GBDT中就是一个个决策树。要得到最终结果，只需要把初始值或者初始的函数加上每次的增量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 XGBoost\n",
    "XGBoost是Exterme Gradient Boosting（极限梯度提升）的缩写，它是基于决策树的集成机器学习算法，它以梯度提升（Gradient Boost）为框架。XGBoost是由由GBDT发展而来，同样是利用加法模型与前向分步算法实现学习的优化过程。XGBoost的可以使用CART作为基学习器，也可以使用线性分类器作为基学习器。以CART作为基学习器时，其决策规则和决策树是一样的，但CART的每一个叶节点具有一个权重，也就是叶节点的得分或者说是叶节点的预测值。\n",
    "\n",
    "\n",
    "### XGboost 算法流程：\n",
    "定义目标函数：\n",
    "\n",
    "$$\n",
    "Obj^{(t)}=损失函数+正则项=\\sum_{i=1}^{n}L(y_i,\\hat{y}_i^t)+\\sum_{i}^{t}\\Omega(f_i)=\\sum_{i=1}^{n}L(y_i,\\hat{y}_i^{t-1}+f_t(x_i))+\\Omega(f_t)+constant\n",
    "$$\n",
    "\n",
    "引入二阶泰勒展开：\n",
    "$$\n",
    "f(x+\\Delta x) \\approx f(x)+f^`(x)\\Delta x+f^{``}(x) \\Delta x^2\n",
    "$$\n",
    "\n",
    "定义 $g_i=\\partial _{\\hat{y}(t-1)}L(y_i,\\hat{y}^{(t-1)})$ , $h_i=\\partial _{\\hat{y}(t-1)}^2L(y_i,\\hat{y}^{(t-1)})$\n",
    "\n",
    "$$\n",
    "Obj^{(t)}\\approx \\sum _{i=1}^{n} [L(y_i,\\hat{y}^{(t-1)})+g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)+constant\n",
    "$$\n",
    "\n",
    "注意到上式中$L(y_i,\\hat{y}^{(t-1)})$部分表示前t-1次迭代的损失函数，在当前第t次迭代来说已经是一个确定的常数，省略常数项则得到下面的式子:\n",
    "\n",
    "$$\n",
    "Obj^{(t)}\\approx \\sum _{i=1}^{n} [g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)\n",
    "$$\n",
    "\n",
    "可以看出目标函数只依赖于数据点的一阶和二阶导数。\n",
    "\n",
    "\n",
    "在目标函数中，我们用$w_q(x)$表示一棵树，包括了树的结构以及叶子结点的权重，w表示权重（反映预测的概率），q表示样本所在的索引号（反映树的结构）\n",
    "\n",
    "定义正则项,\n",
    "$$\n",
    "\\Omega(f_t)=\\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "\n",
    "定义集合$I_j$为树的第j个叶节点上的所有样本点的集合，即给定一颗树，所有按照决策规则被划分到第j个叶节点的样本集合。\n",
    "\n",
    "$$\n",
    "Obj^{(t)}=\\sum_{j=1}^{T}[(\\sum_{i \\in I_j}g_i)w_j+\\frac{1}{2}(\\sum_{i \\in I_j}(h_i+\\lambda) w_j^2)]+ \\gamma T\n",
    "$$\n",
    "\n",
    "令\n",
    "$$\n",
    "\\frac{\\partial Obj^{(t)}}{\\partial w_j}=0  \\qquad \\Longrightarrow  \\quad (\\sum_{i \\in I_j}g_i)+(\\sum_{i \\in I_j}h_i+\\lambda)w_j=0 \\qquad\n",
    "\\Longrightarrow \\quad w_j^*=-\\frac{\\sum_{i \\in I_j}g_i}{\\sum_{i \\in I_j}h_i+\\lambda}\n",
    "$$  \n",
    "\n",
    "将 $w_j^*$ 代入目标函数,\n",
    "\n",
    "$$\n",
    "Obj^{(t)}=-\\frac{1}{2}\\sum_{j=1}^{T}\\frac{(\\sum_{i \\in I_j}g_i)^2}{\\sum_{i \\in I_j}h_i+\\lambda}\n",
    "$$\n",
    "\n",
    "令 $G_i=\\sum_{i \\in I_j}g_i$, $H_i=\\sum_{i \\in I_j}h_i$, 有 \n",
    "\n",
    "$$\n",
    "Obj^{(t)}=-\\frac{1}{2}\\sum_{j=1}^{T}\\frac{G_j^2}{H_j+\\lambda}+\\gamma T\n",
    "$$\n",
    "\n",
    "在决策树的生成中，我们用ID3、C4.5、Gini指数等指标去选择最优分裂特征、切分点(CART时)，XGBoost同样定义了特征选择和切分点选择的指标：\n",
    "\n",
    "$$\n",
    "Gain=\\frac{1}{2}[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}]-\\gamma\n",
    "$$\n",
    "\n",
    "使用上式判断切分增益，Gain值越大，说明分裂后能使目标函数减少越多，就越好。其中$\\frac{G_L^2}{H_L+\\lambda}$ 表示在某个节点按条件切分后左节点的得分，$\\frac{G_R^2}{H_R+\\lambda}$表示在某个节点按条件切分后右节点的得分， $\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}$表示切分前的得分，$\\gamma$表示切分后模型复杂度的增加量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pic1.zhimg.com/v2-10a2eefefac46c43fc6fba215b6c4684_r.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost和GBDT的区别\n",
    "    1）将树模型的复杂度加入到正则项中，来避免过拟合，因此泛化性能会优于GBDT\n",
    "\n",
    "    2）损失函数是用泰勒展开式展开的，同时用到了一阶导和二阶导，可以加快优化速度\n",
    "\n",
    "    3）和GBDT只支持CART作为基分类器之外，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化\n",
    "\n",
    "    4）引进了特征子采样，像RandomForest那样，这种方法既能降低过拟合，还能减少计算\n",
    "\n",
    "    5）在寻找最佳分割点时，考虑到传统的贪心算法效率较低，实现了一种近似贪心算法，用来加速和减小内存消耗，除此之外还考虑了稀疏数据集和缺失值的处理，对于特征的值有缺失的样本，XGBoost依然能自动找到其要分裂的方向\n",
    "\n",
    "    6）XGBoost支持并行处理，XGBoost的并行不是在模型上的并行，而是在特征上的并行，将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。这个block也使得并行化成为了可能，其次在进行节点分裂时，计算每个特征的增益，最终选择增益最大的那个特征去做分割，那么各个特征的增益计算就可以开多线程进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GBDT 和 Adaboost 的区别\n",
    "    1）Adaboost强调Adaptive（自适应），通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting。\n",
    "    而GBDT则是旨在不断减少残差（回归），通过不断加入新的树，旨在残差减少（负梯度）的方向上建立一个新的模型。——即损失函数是旨在最快速度降低残差。\n",
    "    \n",
    "    2）而XGBoost的boosting策略则与GBDT类似，区别在于GBDT旨在通过不断加入新的树最快速度降低残差，而XGBoost则可以人为定义损失函数（可以是最小平方差、logistic loss function、hinge loss function或者人为定义的loss function），只需要知道该loss function对参数的一阶、二阶导数便可以进行boosting，其进一步增大了模型的泛华能力，其贪婪法寻找添加树的结构以及loss function中的损失函数与正则项等一系列策略也使得XGBoost预测更准确。\n",
    "    \n",
    "ref:\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/34534004\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/75217528\n",
    "\n",
    "https://www.cnblogs.com/chaoren399/p/4847462.html\n",
    "\n",
    "https://blog.csdn.net/gumpeng/article/details/51397737\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/56137208\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 聚类\n",
    "3.1 K 均值聚类\n",
    "\n",
    "K 均值聚类是一种通用目的的算法，聚类的度量基于样本点之间的几何距离（即在坐标平面中的距离）。集群是围绕在聚类中心的族群，而集群呈现出类球状并具有相似的大小。聚类算法是我们推荐给初学者的算法，因为该算法不仅十分简单，而且还足够灵活以面对大多数问题都能给出合理的结果。\n",
    "\n",
    "\n",
    "    优点：K 均值聚类是最流行的聚类算法，因为该算法足够快速、简单，并且如果你的预处理数据和特征工程十分有效，那么该聚类算法将拥有令人惊叹的灵活性。\n",
    "\n",
    "    缺点：该算法需要指定集群的数量，而 K 值的选择通常都不是那么容易确定的。另外，如果训练数据中的真实集群并不是类球状的，那么 K 均值聚类会得出一些比较差的集群。\n",
    "\n",
    "    Python 实现：http://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "\n",
    "    R 实现：https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html\n",
    "\n",
    "3.2 Affinity Propagation 聚类\n",
    "\n",
    "AP 聚类算法是一种相对较新的聚类算法，该聚类算法基于两个样本点之间的图形距离（graph distances）确定集群。采用该聚类方法的集群拥有更小和不相等的大小。\n",
    "\n",
    "    优点：该算法不需要指出明确的集群数量（但是需要指定「sample preference」和「damping」等超参数）。\n",
    "\n",
    "    缺点：AP 聚类算法主要的缺点就是训练速度比较慢，并需要大量内存，因此也就很难扩展到大数据集中。另外，该算法同样假定潜在的集群是类球状的。\n",
    "\n",
    "    Python 实现：http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation\n",
    "\n",
    "    R 实现：https://cran.r-project.org/web/packages/apcluster/index.html\n",
    "\n",
    "3.3 层次聚类（Hierarchical / Agglomerative）\n",
    "\n",
    "层次聚类是一系列基于以下概念的聚类算法：\n",
    "\n",
    "    最开始由一个数据点作为一个集群\n",
    "\n",
    "    对于每个集群，基于相同的标准合并集群\n",
    "\n",
    "    重复这一过程直到只留下一个集群，因此就得到了集群的层次结构。\n",
    "\n",
    "\n",
    "    优点：层次聚类最主要的优点是集群不再需要假设为类球形。另外其也可以扩展到大数据集。\n",
    "\n",
    "    缺点：有点像 K 均值聚类，该算法需要设定集群的数量（即在算法完成后需要保留的层次）。\n",
    "\n",
    "    Python 实现：http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering\n",
    "\n",
    "    R 实现：https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html\n",
    "\n",
    "3.4 DBSCAN\n",
    "\n",
    "DBSCAN 是一个基于密度的算法，它将样本点的密集区域组成一个集群。最近还有一项被称为 HDBSCAN 的新进展，它允许改变密度集群。\n",
    "\n",
    "    优点：DBSCAN 不需要假设集群为球状，并且它的性能是可扩展的。此外，它不需要每个点都被分配到一个集群中，这降低了集群的异常数据。\n",
    "\n",
    "    缺点：用户必须要调整「epsilon」和「min_sample」这两个定义了集群密度的超参数。DBSCAN 对这些超参数非常敏感。\n",
    "\n",
    "    Python 实现：http://scikit-learn.org/stable/modules/clustering.html#dbscan\n",
    "\n",
    "    R 实现：https://cran.r-project.org/web/packages/dbscan/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref :\n",
    "https://zhuanlan.zhihu.com/p/37381630"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mean算法思路\n",
    "算法步骤：\n",
    "\n",
    "    1.（随机）选择K个聚类的初始中心；\n",
    "    \n",
    "    2.对任意一个样本点，求其到K个聚类中心的距离，将样本点归类到距离最小的中心的聚类，如此迭代n次；\n",
    "    \n",
    "    3.每次迭代过程中，利用均值等方法更新各个聚类的中心点(质心)；\n",
    "    \n",
    "    4.对K个聚类中心，利用2,3步迭代更新后，如果位置点变化很小(可以设置阈值)，则认为达到稳定状态，迭代结束，对不同的聚类块和聚类中心可选择不同的颜色标注。\n",
    "\n",
    "优点 1）原理比较简单，实现也是很容易，收敛速度快。 2）聚类效果较优。 3）算法的可解释度比较强。 4）主要需要调参的参数仅仅是簇数k。\n",
    "\n",
    "缺点 1）K值的选取不好把握 2）对于不是凸的数据集比较难收敛 3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。 4） 最终结果和初始点的选择有关，容易陷入局部最优。5） 对噪音和异常点比较的敏感。\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://pic4.zhimg.com/v2-bce5f0ed9246f16ca71e18e06ee609a7_b.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### K-Mean 的K 值确定\n",
    "\n",
    "1.手肘法：\n",
    "核心指标：SSE(sum of the squared errors , 误差平方和)\n",
    "\n",
    "$$\n",
    "SSE=\\sum_{i=1}^{k}\\sum_{p\\in C_i}|p-m_i|^2 \n",
    "$$\n",
    "其中，$C_i$是第 i个簇, p是$C_i$中的样本点,$m_i$是$C_i$的质心（样本均值）,SSE是所有样本的聚类误差，衡量聚类效果的好坏\n",
    "\n",
    "    1.随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。\n",
    "    2.当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。\n",
    "\n",
    "<img src=\"https://img-blog.csdn.net/20180428235006200?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI5OTU3NDU1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width=\"400\" height=\"200\" >\n",
    "\n",
    "2.轮廓系数法：\n",
    "\n",
    "使用轮廓系数(silhouette coefficient)来确定，选择使系数较大所对应的k值\n",
    "方法：\n",
    "\n",
    "计算样本i到同簇其他样本的平均距离ai。ai越小，说明样本i越应该被聚类到该簇。将ai称为样本i的簇内不相似度。簇C中所有样本的ai均值称为簇C的簇不相似度。\n",
    "\n",
    "计算样本i到其他某簇Cj的所有样本的平均距离$b_{ij}$，称为样本i与簇Cj的不相似度。定义为样本i的簇间不相似度：bi=min{bi1, bi2, ..., bik}， bi越大，说明样本i越不属于其他簇。\n",
    "\n",
    "根据样本i的簇内不相似度ai和簇间不相似度bi，定义样本i的轮廓系数：\n",
    "\n",
    "$$\n",
    "s(i)=\\frac{b(i)-a(i)}{max\\{a(i),b(i)\\}}\n",
    "$$\n",
    "\n",
    "判断：\n",
    "\n",
    "    轮廓系数范围在[-1,1]之间。该值越大，越合理。\n",
    "\n",
    "    si接近1，则说明样本i聚类合理；\n",
    "\n",
    "    si接近-1，则说明样本i更应该分类到另外的簇；\n",
    "\n",
    "    若si 近似为0，则说明样本i在两个簇的边界上。\n",
    "\n",
    "    所有样本的s i 的均值称为聚类结果的轮廓系数，是该聚类是否合理、有效的度量。\n",
    "\n",
    "    使用轮廓系数(silhouette coefficient)来确定，选择使系数较大所对应的k值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://10.30.110.17:3838/Growth/HotelModel/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 分类算法\n",
    "本篇将讲述三类常用的分类算法：\n",
    "\n",
    "逻辑回归   Logistic Regression（LR） \n",
    "\n",
    "线性/二次判别分析   Linear Discriminant Analysis（LDA） / Quadratic Discriminant Analysis（QDA）\n",
    "\n",
    "K最近邻法   K-nearest neighbors（KNN） \n",
    "\n",
    "     首先，本文将从整体上来比较这三类分类算法：\n",
    "\n",
    "    （1） 当真实的分类边界是线性时，LR和LDA通常会表现更好；当真实的分类边界是一般的非线性时，QDA通常会表现更好；当真实的分类边界\n",
    "\n",
    "             更为复杂时， KNN 通常会表现的更好。\n",
    "\n",
    "    （2） LR和LDA 都将产生线性分类边界，两种唯一的不同是LR的系数估计是通过极大似然法，而LDA的系数是运用正态分布的均值和方差的估计\n",
    "\n",
    "             值计算得到的；LR适用于二分类问题，对于多分类问题，LDA则更为常见。\n",
    "\n",
    "    （3） 当样本能够被完全线性分类时，LR的参数估计将会出现不稳定情况，此时LR方法不适用（详细见另一篇博文 广义线性模型 R--glm函数），\n",
    "\n",
    "             LDA参数估计则不会出现这种情况；当样本量较小且自变量的分布近似于状态分布时，LDA会比LR表现的更好。\n",
    "\n",
    "    （4） LDA和QDA都是建立在自变量服从正态分布的假设上，所以当自变量的分布确实是几乎服从正态分布时，这两种方法会表现的较好；\n",
    "\n",
    "             LDA和QDA 的区别在于LDA假设所有类别的自变量都服从方差相同的正态分布，而QDA假设对于因变量属于不同类别的自变量服从方差不\n",
    "\n",
    "             同的正态分布，选择LDA和QDA的关键在于偏置--方差的权衡。\n",
    "\n",
    "    （5） QDA可以认为是非参数学习法KNN和具有线性分类边界的LR及LDA方法的折中，QDA假设分类边界是二次的，所以它能够比线性模型更准\n",
    "\n",
    "             确的为更多的问题建立模型；同时由于其二次边界的额外假设，当样本量较小时，其能够优于KNN方法。\n",
    "\n",
    "    （6） KNN是非参数学习方法，因此它不对分类边界做任何假设，当分类边界为高度非线性是便会优于RL和LDA，但是其不能反映每个自变量对因\n",
    "\n",
    "             变量的影响。\n",
    "\n",
    "    （7） LR可以适用于含有定性自变量的模型，对于定性的自变量LR会将其转换为虚拟变量，但是LDA、QDA及KNN都不适用于定性的自变量。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
