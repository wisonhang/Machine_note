{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 4624 characters\n"
     ]
    }
   ],
   "source": [
    "path_to_file='ios Tracks 20191020.xlsx'\n",
    "text = pd.read_excel(path_to_file)\n",
    "\n",
    "# 文本长度是指文本中的字符个数\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "text_data='>'.join(text['track'].apply(lambda x: re.compile('F16.*').sub('F16;',x )))\n",
    "text_array=text_data.split('>')\n",
    "vocab=sorted(set(text_array))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A00\n",
      "O00\n",
      "A00\n",
      "O00\n",
      "A00\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# 创建训练样本 / 目标\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A00O00A00O00A00A06F0BF16;A00A08F0BF16;A00A01B00B01D00D03F0AD00B00B01D00D03F0AD00B00B01D00B00A00A01B00B01D00D03F0AD00D03F0AF16;A00A04F0BA00O00A00P00A00A01B00B99B00A00A06F0BA00P00O00A00A06F0BF16;A00A04F0BF16;A00A02P00A00P00F0CP00A00A02C00A00O00A00P00F0CF16;A00A04F0BF16;A00A04F0BF16;A00A04F0BA00A06F0BA00A06F0BA00'\n",
      "'A06F0BF16;A00A04F0BF16;A00O00A00A03D00D03F0AD00A00A03D00D03F0AF16;A00A03D00D03F0AD00D03F0AF16;A00P00A00A03D00D03F0AD00D03F0AF16;A00O00A00A03D00D03F0AD00D03F0AF16;A00A02C00C99F0DF16;A00A02C00C99F0DF16;A00A01B00B01D00D03F0AD00D03F0AD00D03F0AF16;A00P00A00O00A00A04F0BF16;A00A04F0BA00A06F0BA00A06F0BA00A02C00C99D00D03'\n",
      "'F0AD00D03F0AF16;A00A02C00C99F0DF16;A00O00A00P00A00P00A00P00A00A06F0BA00A06F0BA00A06F0BA00A06F0BF16;A00A06F0BF16;A00A06F0BA00A06F0BF16;A00O00A00P00A00A04F0BA00P00O00A00P00A00O00A00A04F0BF16;A06F0BA00O00A00P00A00P00O00A00A06F0BA00A06F0BA00A06F0BA00A06F0BA00A06F0BA00A06F0BA00A06F0BA00P00A00A04F0BA00P00O00A00O00'\n",
      "'A00O00A00A08F0BF16;C00C99F0DF16;A00A04F0BF16;A00A03D00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AF16;A00A06F0BF16;O00A00D00A00A04F0BA00P00A00A04F0BF16;A00A04F0BA00D00D03F0AF16;A00D00D03F0AF16;A00A02C00D00D03F0AD00D03F0AD00D03F0AF16;'\n",
      "'A00A06F0BA00A02C00C99F0DC00A00A06F0BF16;A00A04F0BA00A04F0BF16;A00A04F0BF16;A00A04F0BA00A01B00B01D00D03F0AF16;A00A06F0BF16;A00A02C00C99D00D03F0AF16;A00A02C00A00A02C00A00A06F0BF16;A00A02C00A00A01B00B01D00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D03F0AD00D04F0AD00D03F0AF16;D00D03F0AF16;A00P00A00P00O00'\n",
      "Input data:  'A00O00A00O00A00A06F0BF16;A00A08F0BF16;A00A01B00B01D00D03F0AD00B00B01D00D03F0AD00B00B01D00B00A00A01B00B01D00D03F0AD00D03F0AF16;A00A04F0BA00O00A00P00A00A01B00B99B00A00A06F0BA00P00O00A00A06F0BF16;A00A04F0BF16;A00A02P00A00P00F0CP00A00A02C00A00O00A00P00F0CF16;A00A04F0BF16;A00A04F0BF16;A00A04F0BA00A06F0BA00A06F0B'\n",
      "Target data: 'O00A00O00A00A06F0BF16;A00A08F0BF16;A00A01B00B01D00D03F0AD00B00B01D00D03F0AD00B00B01D00B00A00A01B00B01D00D03F0AD00D03F0AF16;A00A04F0BA00O00A00P00A00A01B00B99B00A00A06F0BA00P00O00A00A06F0BF16;A00A04F0BF16;A00A02P00A00P00F0CP00A00A02C00A00O00A00P00F0CF16;A00A04F0BF16;A00A04F0BF16;A00A04F0BA00A06F0BA00A06F0BA00'\n",
      "Input data:  'A06F0BF16;A00A04F0BF16;A00O00A00A03D00D03F0AD00A00A03D00D03F0AF16;A00A03D00D03F0AD00D03F0AF16;A00P00A00A03D00D03F0AD00D03F0AF16;A00O00A00A03D00D03F0AD00D03F0AF16;A00A02C00C99F0DF16;A00A02C00C99F0DF16;A00A01B00B01D00D03F0AD00D03F0AD00D03F0AF16;A00P00A00O00A00A04F0BF16;A00A04F0BA00A06F0BA00A06F0BA00A02C00C99D00'\n",
      "Target data: 'F0BF16;A00A04F0BF16;A00O00A00A03D00D03F0AD00A00A03D00D03F0AF16;A00A03D00D03F0AD00D03F0AF16;A00P00A00A03D00D03F0AD00D03F0AF16;A00O00A00A03D00D03F0AD00D03F0AF16;A00A02C00C99F0DF16;A00A02C00C99F0DF16;A00A01B00B01D00D03F0AD00D03F0AD00D03F0AF16;A00P00A00O00A00A04F0BF16;A00A04F0BA00A06F0BA00A06F0BA00A02C00C99D00D03'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#If your program depends on the batches having the same outer dimension, \n",
    "#you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in  dataset.take(2):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 5 ('A06')\n",
      "  expected output: 26 ('F0B')\n",
      "Step    1\n",
      "  input: 26 ('F0B')\n",
      "  expected output: 31 ('F16;')\n",
      "Step    2\n",
      "  input: 31 ('F16;')\n",
      "  expected output: 0 ('A00')\n",
      "Step    3\n",
      "  input: 0 ('A00')\n",
      "  expected output: 4 ('A04')\n",
      "Step    4\n",
      "  input: 4 ('A04')\n",
      "  expected output: 26 ('F0B')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批大小\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 设定缓冲区大小，以重新排列数据集\n",
    "# （TF 数据被设计为可以处理可能是无限的序列，\n",
    "# 所以它不会试图在内存中重新排列整个序列。相反，\n",
    "# 它维持一个缓冲区，在缓冲区重新排列元素。） \n",
    "BUFFER_SIZE = 10000\n",
    "###representing the number of elements from this dataset from which the new dataset will sample.\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "#shuffle是防止数据过拟合的重要手段\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_gru at 0x7fa118c1b2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa118c1b2f0>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x7fa118c1b2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa118c1b2f0>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x7fa118c1b598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa118c1b598>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x7fa118c1b598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa118c1b598>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "# 词集的长度\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 嵌入的维度\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN 的单元数量\n",
    "rnn_units = 1024\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 38) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           9728      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 38)            38950     \n",
      "=================================================================\n",
      "Total params: 3,986,982\n",
      "Trainable params: 3,986,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'A06F0BF16;A00O00A00P00A00P00F0CP00A00D00D03F0AD00A00P00A00P00O00A00P00A00P00A00P00A00P00A00D00D03F0AF16;F16;A00D00D03F0AD00D03F0AD00D03F0AD00D03F0AF16;O00A00P00O00A00A06F0BF16;A00A04F0BA00A06F0BA00A01B00B01D00D03F0AD00D02B00B01D00D03F0AD00D03F0AF16;A00A02C00C99F0DC00A00O00A00A04F0BA00A04F0BA00F0BA00A04F0B'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'F0IA02A12A08A06D10O00D11F0AE02D10D04A06E02E02A06D01B99D02O04F0AO00B00B99A06F0AF0DA00A10D04D01D02F0IC99A06D03A02A01F0FA04A08D10F0AF0CD10D10D00E02D00F0AF0BO00A11D10F0ID01D10A10O04B01P01P00D11D00P01A11A00F0BD11D03A04A08A06D12D03A02E00D10D01D04F0CA03A03F0AA03B01D12A03A08D11O02F0FA08O04F0DA10F0CC00P02A10'\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 38)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       3.6374\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "\n",
    "# 检查点保存至的目录\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# 检查点的文件名\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 5s 776ms/step - loss: 3.7665\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 5s 696ms/step - loss: 2.9187\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 5s 703ms/step - loss: 2.4582\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 5s 702ms/step - loss: 1.9103\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 5s 768ms/step - loss: 1.5152\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 5s 764ms/step - loss: 1.2298\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 5s 767ms/step - loss: 1.0827\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 5s 763ms/step - loss: 0.9967\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 5s 772ms/step - loss: 0.9376\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 5s 772ms/step - loss: 0.9005\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 5s 772ms/step - loss: 0.8772\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 5s 768ms/step - loss: 0.8611\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 5s 704ms/step - loss: 0.8495\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 5s 702ms/step - loss: 0.8406\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 5s 705ms/step - loss: 0.8338\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 5s 740ms/step - loss: 0.8281\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 6s 815ms/step - loss: 0.8234\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 5s 755ms/step - loss: 0.8193\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 5s 768ms/step - loss: 0.8156\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 5s 769ms/step - loss: 0.8122\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_gru at 0x7fa118c1b2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa118c1b2f0>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x7fa118c1b2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa118c1b2f0>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x7fa118c1b598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa118c1b598>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x7fa118c1b598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa118c1b598>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            9728      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 38)             38950     \n",
      "=================================================================\n",
      "Total params: 3,986,982\n",
      "Trainable params: 3,986,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # 评估步骤（用学习过的模型生成文本）\n",
    "\n",
    "  # 要生成的字符个数\n",
    "    num_generate = 100\n",
    "\n",
    "  # 将起始字符串转换为数字（向量化）\n",
    "    input_eval = [char2idx[start_string]]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # 空字符串用于存储结果\n",
    "    text_generated = []\n",
    "\n",
    "      # 低温度会生成更可预测的文本\n",
    "      # 较高温度会生成更令人惊讶的文本\n",
    "      # 可以通过试验以找到最好的设定\n",
    "    temperature = 1.0\n",
    "\n",
    "    # 这里批大小为 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # 删除批次的维度\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # 用分类分布预测模型返回的字符\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        if idx2char[predicted_id]=='F16;':\n",
    "            break\n",
    "    return (start_string +'>'+ '>'.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n"
     ]
    }
   ],
   "source": [
    "path_list=[]\n",
    "for i in range(100000):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    path_list.append(generate_text(model,'A00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AA=pd.DataFrame(pd.value_counts(path_list),columns=['count']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>11038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A00&gt;A04&gt;F0B&gt;F16;</td>\n",
       "      <td>10362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A00&gt;A02&gt;C00&gt;C99&gt;F0D&gt;F16;</td>\n",
       "      <td>3947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A00&gt;A08&gt;F0B&gt;F16;</td>\n",
       "      <td>2664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A00&gt;A03&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>2166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>A00&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>A00&gt;A06&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>A00&gt;P00&gt;F0C&gt;F16;</td>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>A00&gt;A02&gt;C00&gt;C99&gt;F0D&gt;C00&gt;C99&gt;F0D&gt;F16;</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>A00&gt;A12&gt;F0B&gt;F16;</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>A00&gt;A11&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>A00&gt;A01&gt;B00&gt;B01&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>A00&gt;F0B&gt;F16;</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>A00&gt;F0C&gt;F16;</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>A00&gt;O00&gt;A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>A00&gt;O00&gt;A00&gt;A04&gt;F0B&gt;F16;</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>A00&gt;A04&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>A00&gt;A01&gt;B00&gt;B99&gt;F16;</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>A00&gt;A04&gt;F0B&gt;A00&gt;A04&gt;F0B&gt;F16;</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>A00&gt;A03&gt;D00&gt;D03&gt;F0A&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>A00&gt;A06&gt;F0B&gt;A00&gt;A04&gt;F0B&gt;F16;</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>A00&gt;A06&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>A00&gt;A02&gt;C00&gt;C99&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>A00&gt;A08&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>A00&gt;A02&gt;C00&gt;C99&gt;F0D&gt;C00&gt;C99&gt;F0D&gt;C00&gt;C99&gt;F0D&gt;F16;</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>A00&gt;O00&gt;A00&gt;A02&gt;C00&gt;C99&gt;F0D&gt;F16;</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>A00&gt;A01&gt;B00&gt;B99&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>A00&gt;F16;</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>A00&gt;O00&gt;P01&gt;F16;</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>A00&gt;P00&gt;F0C&gt;P00&gt;F0C&gt;F16;</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>A00&gt;A03&gt;D00&gt;D03&gt;F0A&gt;D00&gt;D03&gt;F0A&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>A00&gt;P01&gt;F16;</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>A00&gt;A01&gt;B00&gt;B01&gt;D00&gt;D03&gt;F0A&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>A00&gt;F0D&gt;F16;</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>A00&gt;A02&gt;B00&gt;B99&gt;F16;</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>A00&gt;A04&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>A00&gt;D00&gt;D03&gt;F0A&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>A00&gt;O00&gt;A00&gt;P00&gt;F0C&gt;F16;</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>A00&gt;A06&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;A00&gt;A06&gt;F0...</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>A00&gt;P02&gt;F0C&gt;F16;</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>A00&gt;A02&gt;C00&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>A00&gt;P00&gt;P01&gt;F16;</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>A00&gt;P00&gt;A04&gt;F0B&gt;F16;</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>A00&gt;A08&gt;F0B&gt;A00&gt;A04&gt;F0B&gt;F16;</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>A00&gt;C99&gt;F0D&gt;F16;</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>A00&gt;O00&gt;F0C&gt;F16;</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>A00&gt;A03&gt;D00&gt;D03&gt;F0A&gt;D00&gt;D03&gt;F0A&gt;D00&gt;D03&gt;F0A&gt;D0...</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>A00&gt;P00&gt;P02&gt;F0C&gt;F16;</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>A00&gt;F0B&gt;A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>A00&gt;P00&gt;A00&gt;A04&gt;F0B&gt;F16;</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>A00&gt;A01&gt;B00&gt;D00&gt;D03&gt;F0A&gt;F16;</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>A00&gt;A06&gt;F0B&gt;F0B&gt;F16;</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>A00&gt;A03&gt;D00&gt;D01&gt;C00&gt;C99&gt;F0D&gt;F16;</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>A00&gt;F0C&gt;F0B&gt;F16;</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>A00&gt;P00&gt;A00&gt;A06&gt;F0B&gt;F16;</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>A00&gt;P00&gt;F0C&gt;F0C&gt;F16;</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>A00&gt;A02&gt;C00&gt;C99&gt;F0D&gt;C00&gt;C99&gt;F0D&gt;C00&gt;C99&gt;F0D&gt;C0...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                index  count\n",
       "0                                    A00>A06>F0B>F16;  11038\n",
       "1                                    A00>A04>F0B>F16;  10362\n",
       "2                            A00>A02>C00>C99>F0D>F16;   3947\n",
       "3                                    A00>A08>F0B>F16;   2664\n",
       "4                            A00>A03>D00>D03>F0A>F16;   2166\n",
       "5                                A00>D00>D03>F0A>F16;   1197\n",
       "6                        A00>A06>F0B>A00>A06>F0B>F16;   1087\n",
       "7                                    A00>P00>F0C>F16;    930\n",
       "8                A00>A02>C00>C99>F0D>C00>C99>F0D>F16;    841\n",
       "9                                    A00>A12>F0B>F16;    773\n",
       "10                           A00>A11>D00>D03>F0A>F16;    607\n",
       "11                   A00>A01>B00>B01>D00>D03>F0A>F16;    599\n",
       "12                                       A00>F0B>F16;    559\n",
       "13                                       A00>F0C>F16;    531\n",
       "14                           A00>O00>A00>A06>F0B>F16;    522\n",
       "15                           A00>O00>A00>A04>F0B>F16;    522\n",
       "16                       A00>A04>F0B>A00>A06>F0B>F16;    485\n",
       "17                               A00>A01>B00>B99>F16;    366\n",
       "18                       A00>A04>F0B>A00>A04>F0B>F16;    353\n",
       "19               A00>A03>D00>D03>F0A>D00>D03>F0A>F16;    324\n",
       "20                       A00>A06>F0B>A00>A04>F0B>F16;    322\n",
       "21           A00>A06>F0B>A00>A06>F0B>A00>A06>F0B>F16;    308\n",
       "22                   A00>A02>C00>C99>D00>D03>F0A>F16;    300\n",
       "23                       A00>A08>F0B>A00>A06>F0B>F16;    277\n",
       "24   A00>A02>C00>C99>F0D>C00>C99>F0D>C00>C99>F0D>F16;    276\n",
       "25                   A00>O00>A00>A02>C00>C99>F0D>F16;    259\n",
       "26                   A00>A01>B00>B99>D00>D03>F0A>F16;    227\n",
       "27                                           A00>F16;    224\n",
       "28                                   A00>O00>P01>F16;    216\n",
       "29                           A00>P00>F0C>P00>F0C>F16;    198\n",
       "30   A00>A03>D00>D03>F0A>D00>D03>F0A>D00>D03>F0A>F16;    197\n",
       "31                                       A00>P01>F16;    188\n",
       "32       A00>A01>B00>B01>D00>D03>F0A>D00>D03>F0A>F16;    185\n",
       "33                                       A00>F0D>F16;    180\n",
       "34                               A00>A02>B00>B99>F16;    178\n",
       "35           A00>A04>F0B>A00>A06>F0B>A00>A06>F0B>F16;    174\n",
       "36                   A00>D00>D03>F0A>D00>D03>F0A>F16;    170\n",
       "37                           A00>O00>A00>P00>F0C>F16;    168\n",
       "38  A00>A06>F0B>A00>A06>F0B>A00>A06>F0B>A00>A06>F0...    158\n",
       "39                                   A00>P02>F0C>F16;    153\n",
       "40                       A00>A02>C00>D00>D03>F0A>F16;    144\n",
       "41                                   A00>P00>P01>F16;    144\n",
       "42                               A00>P00>A04>F0B>F16;    141\n",
       "43                       A00>A08>F0B>A00>A04>F0B>F16;    137\n",
       "44                                   A00>C99>F0D>F16;    131\n",
       "45                                   A00>O00>F0C>F16;    130\n",
       "46  A00>A03>D00>D03>F0A>D00>D03>F0A>D00>D03>F0A>D0...    123\n",
       "47                               A00>P00>P02>F0C>F16;    123\n",
       "48                           A00>F0B>A00>A06>F0B>F16;    121\n",
       "49                           A00>P00>A00>A04>F0B>F16;    117\n",
       "50                       A00>A01>B00>D00>D03>F0A>F16;    116\n",
       "51                               A00>A06>F0B>F0B>F16;    113\n",
       "52                   A00>A03>D00>D01>C00>C99>F0D>F16;    113\n",
       "53                                   A00>F0C>F0B>F16;    110\n",
       "54                           A00>P00>A00>A06>F0B>F16;    110\n",
       "55                               A00>P00>F0C>F0C>F16;    106\n",
       "56  A00>A02>C00>C99>F0D>C00>C99>F0D>C00>C99>F0D>C0...    103"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AA.loc[AA['count']>100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
